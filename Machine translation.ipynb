{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QeqVEoQ_tirJ"
   },
   "source": [
    "지금까지는 input과 output이 1대1로 매핑되는 시나리오를 보았다.\n",
    "\n",
    "이번 절에서는 모든 input 데이터를 vector로 매핑한 다음 output vector로 decoding하는 결과를 가져오는 아키텍처를 구축하는 방법을 살펴본다.\n",
    "\n",
    "영어로 입력된 텍스트를 프랑스어로 번역하는 예제로 볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4HSapX1ouGGE"
   },
   "source": [
    "# Getting ready\n",
    "\n",
    "기계번역을 수행하기 위해 정의하게 될 아키텍처는 다음과 같다.\n",
    "\n",
    "- 입력 문장과 해당 프랑스어 번역을 사용할 수 있는 레이블이 지정된 데이터 세트 가져오기\n",
    "- 영어와 프랑스어 텍스트 각각에 자주 쓰이는 단어를 토큰화하고 추출한다.\n",
    "  - 빈번한 단어를 식별하기 위해, 우리는 각 단어의 빈도를 센다.\n",
    "  - 모든 단어의 전체 누적 빈도의 상위 80%를 구성하는 단어는 자주 사용되는 단어로 간주한다.\n",
    "- 자주 사용하는 단어에 포함되지 않은 단어들은 unk(unknown) 기호로 대체한다.\n",
    "- 각 단어에 ID를 할당한다.\n",
    "- 입력 텍스트의 벡터를 fetch하는 인코더 LSTM을 작성\n",
    "- 인코딩된 벡터를 dense layer를 통해 전달하여 각 time step에서 디코딩된 텍스트의 확률을 추출할 수 있다.\n",
    "- model fit를 통해 ouput에서의 loss를 최소화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XRh7C5TKwBOy"
   },
   "source": [
    "# How to do it...\n",
    "\n",
    "입력 텍스트를 번역하는 데 도움이 되는 모델 아키텍처가 여러개 있을 수 있다.\n",
    "\n",
    "우리는 그 중 몇가지를 살펴볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E0xYaokYwQVW"
   },
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "입력 및 출력 데이터를 모델에 전달하려면 데이터셋을 전처리 해야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dP1BKm8Ywh7I"
   },
   "source": [
    "### 1. Import the relevant packages and dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDghoZaJwpUh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from string import digits\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HpmDJJLbwvC8"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/2vag8w6yov9c1qz/english%20to%20french.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2437,
     "status": "ok",
     "timestamp": 1563445584399,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "8gSB1DeSw7RN",
    "outputId": "3a4a7489-9b3e-48cc-db00-75304dc0442f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " atis.dict.intent.csv   atis.test.query.csv     atis.train.slots.csv\n",
      " atis.dict.slots.csv    atis.test.slots.csv     atis.zip\n",
      " atis.dict.vocab.csv    atis.train.intent.csv  'english to french.txt'\n",
      " atis.test.intent.csv   atis.train.pkl\t        sample_data\n",
      " atis.test.pkl\t        atis.train.query.csv\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 976,
     "status": "ok",
     "timestamp": 1563445588632,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "VR3SGRXFw0N4",
    "outputId": "af078648-396e-4e54-aef1-19e50dc5aa85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "lines= pd.read_table('english to french.txt', names=['eng', 'fr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 688,
     "status": "ok",
     "timestamp": 1563445590786,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "1QCP7fazxI3l",
    "outputId": "f3a9cb24-17c5-4624-eee7-d540000d13cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149861"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5w8_apoxGa3"
   },
   "source": [
    "### 2. 데이터셋에 약 15만 개의 문장이 있는 것을 감안하여, 모델 구축하기 위한 첫 5만개 문장-번역 쌍만을 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5XioLi47xBI5"
   },
   "outputs": [],
   "source": [
    "lines = lines[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fire!</td>\n",
       "      <td>Au feu !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     eng          fr\n",
       "0    Go.        Va !\n",
       "1   Run!     Cours !\n",
       "2   Run!    Courez !\n",
       "3   Wow!  Ça alors !\n",
       "4  Fire!    Au feu !"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tf_Tqyp5xa9-"
   },
   "source": [
    "### 3. 입력 및 출력 텍스트를 소문자로 변환하고 구두점을 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OSP5Zi6bxebY"
   },
   "outputs": [],
   "source": [
    "lines.eng=lines.eng.apply(lambda x: x.lower())\n",
    "lines.fr=lines.fr.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlhvV_vFxgVh"
   },
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)\n",
    "lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines.fr=lines.fr.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k06AhMLVx4F0"
   },
   "source": [
    "### 4. 출력 문장에서 'start' 토큰과 'end' 토큰을 추가한다.(프랑스 문장)\n",
    "\n",
    "인코더 디코더 아키텍처에 유용할 수 있도록 'start' 토큰과 'end' 토큰를 추가한다.\n",
    "\n",
    "그 이유는 뒤 섹션에서 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHucCBp6yVuh"
   },
   "outputs": [],
   "source": [
    "lines.fr = lines.fr.apply(lambda x : 'start '+ x + ' end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 608,
     "status": "ok",
     "timestamp": 1563445608883,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "XF8A88qZ1Gkx",
    "outputId": "ee1b1c71-43b2-438c-f579-0bf0a7a66578"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go</td>\n",
       "      <td>start va  end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>run</td>\n",
       "      <td>start cours  end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>run</td>\n",
       "      <td>start courez  end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wow</td>\n",
       "      <td>start ça alors  end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fire</td>\n",
       "      <td>start au feu  end</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    eng                   fr\n",
       "0    go        start va  end\n",
       "1   run     start cours  end\n",
       "2   run    start courez  end\n",
       "3   wow  start ça alors  end\n",
       "4  fire    start au feu  end"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 729,
     "status": "ok",
     "timestamp": 1563445612401,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "mUyeuKQv1MF8",
    "outputId": "943b7c3b-5630-4b91-a0d3-dbc19aeae5cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FcNdWN6fyk5U"
   },
   "source": [
    "### frequent 단어를 식별한다.\n",
    "\n",
    "모든 단어의 전체 빈도 80%를 구성하는 빈도를 가진 단어를 frequent 단어로 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oHsGPLid1Pee"
   },
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = create_tokenizer(lines.eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras_preprocessing.text.Tokenizer"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eng_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CkrLgbWq1R53"
   },
   "outputs": [],
   "source": [
    "eng_tokenizer = create_tokenizer(lines.eng)\n",
    "\n",
    "# json 형태로 변환후 불러오는 작업 : dict 객체로 변환\n",
    "output_dict = json.loads(json.dumps(eng_tokenizer.word_counts))\n",
    "\n",
    "df = pd.DataFrame([output_dict.keys(), output_dict.values()]).T\n",
    "\n",
    "df.columns = ['word','count']\n",
    "\n",
    "df = df.sort_values(by='count',ascending = False)\n",
    "\n",
    "df['cum_count'] = df['count'].cumsum()\n",
    "df['cum_perc'] = df['cum_count']/df['cum_count'].max()\n",
    "final_eng_words = df[df['cum_perc'] < 0.8]['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>cum_count</th>\n",
       "      <th>cum_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>start</td>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "      <td>0.158865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>end</td>\n",
       "      <td>50000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.31773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>je</td>\n",
       "      <td>9886</td>\n",
       "      <td>109886</td>\n",
       "      <td>0.34914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>pas</td>\n",
       "      <td>5758</td>\n",
       "      <td>115644</td>\n",
       "      <td>0.367435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>de</td>\n",
       "      <td>4663</td>\n",
       "      <td>120307</td>\n",
       "      <td>0.382251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  count cum_count  cum_perc\n",
       "0   start  50000     50000  0.158865\n",
       "2     end  50000    100000   0.31773\n",
       "20     je   9886    109886   0.34914\n",
       "66    pas   5758    115644  0.367435\n",
       "75     de   4663    120307  0.382251"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['i', 'you', 'a', 'is', 'the'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_eng_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F39DjN4X2DQ2"
   },
   "source": [
    "입력에서 누적적으로 전체 영어 단어의 80%를 구성하는 영어 단어의 수를 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cumsum_80(lang_tokenizer):\n",
    "    # json 형태로 변환후 불러오는 작업 : dict 객체로 변환\n",
    "    output_dict = json.loads(json.dumps(lang_tokenizer.word_counts))\n",
    "\n",
    "    df = pd.DataFrame([output_dict.keys(), output_dict.values()]).T\n",
    "\n",
    "    df.columns = ['word','count']\n",
    "\n",
    "    df = df.sort_values(by='count',ascending = False)\n",
    "\n",
    "    df['cum_count'] = df['count'].cumsum()\n",
    "    df['cum_perc'] = df['cum_count']/df['cum_count'].max()\n",
    "    final_lang_words = df[df['cum_perc'] < 0.8]['word'].values    \n",
    "    return final_lang_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['start', 'end', 'je', 'pas', 'de'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_fr_words = extract_cumsum_80(fr_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['start', 'end', 'je', 'pas', 'de'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_fr_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "POrEe2MX2Zzc"
   },
   "source": [
    "프랑스어 단어도 마찬가지로 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 708,
     "status": "ok",
     "timestamp": 1563445631685,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "BnoxW7UR3XcQ",
    "outputId": "9b2e6680-7bcc-468e-8718-4c0c21e481a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 357\n"
     ]
    }
   ],
   "source": [
    "print(len(final_eng_words),len(final_fr_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J-ZY4i8k2flv"
   },
   "source": [
    "### 6. 빈도가 낮은 단어를 필터링한다.\n",
    "\n",
    "만약 어떤 단어가 자주 쓰이는 단어들 사이에 없다면, 우리는 그것을 알 수 없는 단어로 대체한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RYpM4GuH3UwG"
   },
   "outputs": [],
   "source": [
    "def filter_eng_words(x):\n",
    "    t = []\n",
    "    x = x.split()\n",
    "    for i in range(len(x)):\n",
    "        if x[i] in final_eng_words:\n",
    "            t.append(x[i])\n",
    "        else:\n",
    "            t.append('unk')\n",
    "        \n",
    "    x3 = ''\n",
    "    for i in range(len(t)):\n",
    "        x3 = x3 + t[i] + ' '\n",
    "        \n",
    "    return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fr_words(x):\n",
    "    t = []\n",
    "    x = x.split()\n",
    "    for i in range(len(x)):\n",
    "        if x[i] in final_fr_words:\n",
    "            t.append(x[i])\n",
    "        else:\n",
    "            t.append('unk')\n",
    "        \n",
    "    x3 = ''\n",
    "    for i in range(len(t)):\n",
    "        x3 = x3 + t[i] + ' '  \n",
    "        \n",
    "    return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lang_words(x, final_lang_words):\n",
    "    t = []\n",
    "    x = x.split()\n",
    "    for i in range(len(x)):\n",
    "        if x[i] in final_lang_words:\n",
    "            t.append(x[i])\n",
    "    else:\n",
    "        t.append('unk')\n",
    "        \n",
    "    x3 = ' '.join(t)    \n",
    "    return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zWO7KDRk3lzE"
   },
   "source": [
    "문장을 입력으로 하고, unique 단어를 추출하며, 만약 단어가 빈번한 영어 단어(final_eng_words)  사이에 존재하지 않으면 unk로 대체한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfcWHbU54Ok2"
   },
   "source": [
    "프랑스어도 마찬가지로 처리한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c1ZN9g9M4VUt"
   },
   "source": [
    "예를 들어, 빈번한 단어와 빈번하지 않는 단어를 가진 임의의 문장은 다음과 같이 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1563445649345,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "N7I9Cg1m4RuP",
    "outputId": "76b1f2b2-b9ed-4bf0-f83b-fe3478e2904f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is good unk '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_eng_words('he is extremely good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is good unk'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_lang_words('he is extremely good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_copy = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qx-04FRM4myr"
   },
   "outputs": [],
   "source": [
    "lines['fr'] = lines['fr'].apply(filter_fr_words)\n",
    "lines['eng'] = lines['eng'].apply(filter_eng_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go</td>\n",
       "      <td>start va end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>run</td>\n",
       "      <td>start unk end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>run</td>\n",
       "      <td>start unk end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unk</td>\n",
       "      <td>start ça unk end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unk</td>\n",
       "      <td>start au unk end</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    eng                 fr\n",
       "0   go       start va end \n",
       "1  run      start unk end \n",
       "2  run      start unk end \n",
       "3  unk   start ça unk end \n",
       "4  unk   start au unk end "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_cgLad54qKE"
   },
   "source": [
    "위 처리를 함수로 작성하여 모든 영어와 프랑스어 문장에 대해서 적용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ENsgqF-S4zGA"
   },
   "source": [
    "### 7. 영어(입력) 문장과 프랑스어(출력) 문장에 걸쳐 각 단어에 ID를 할당한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5yTWT0O456U"
   },
   "source": [
    "#### 1) 데이터(영어와 프랑스어 문장)에 모든 unique 단어의 list를 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PGAFo5Fv5ECQ"
   },
   "outputs": [],
   "source": [
    "all_eng_words = set()\n",
    "for eng in lines.eng:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "    \n",
    "all_french_words=set()\n",
    "for fr in lines.fr:\n",
    "    for word in fr.split():\n",
    "        if word not in all_french_words:\n",
    "            all_french_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['was', 'she', 'them', 'bed', 'met']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(all_eng_words)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amis', 'pris', 'largent', 'colère', 'en']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(all_french_words)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t5T1DUB25VYM"
   },
   "outputs": [],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_french_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_french_words)\n",
    "# del all_eng_words, all_french_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'afraid', 'again', 'agree']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'acheté', 'ai', 'aider', 'aije']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1118,
     "status": "ok",
     "timestamp": 1563445682437,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "Qy-veHvA7cyG",
    "outputId": "38b21a13-ceef-453c-93c7-befb6861e9a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk'}"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(all_french_words) - set(final_fr_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 738,
     "status": "ok",
     "timestamp": 1563445688583,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "tADPlhCJ7dFU",
    "outputId": "29953279-98ce-4fe8-bffe-adb7a26a2ad8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_eng_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1563445689599,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "WxC9i1iG7elk",
    "outputId": "a12cec4a-d5ac-4f37-fb32-561230d83858"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCWe1l-55aR_"
   },
   "source": [
    "#### 2) 입력 단어의 사전과 그에 대응하는 index를 생성한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5toKyKZ7hn4"
   },
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict(\n",
    "    [(word, i+1) for i, word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 872,
     "status": "ok",
     "timestamp": 1563445694832,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "P2qS3azH7ids",
    "outputId": "83152125-fe30-4333-da5d-b043a6a1decd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_decoder_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Q5GhqEN7y3f"
   },
   "source": [
    "### 8. 모든 문장의 길이가 동일하도록 입력과 target 문장의 최대 길이를 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenght_list=[]\n",
    "for l in lines.fr:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "fr_max_length = np.max(lenght_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenght_list=[]\n",
    "for l in lines.eng:\n",
    "    lenght_list.append(len(l.split(' ')))\n",
    "eng_max_length = np.max(lenght_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hIGV5a_F8GCC"
   },
   "source": [
    "데이터셋에 대한 여러 아키텍처의 성능을 비교해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6TC7rhj8Qr3"
   },
   "source": [
    "## Traditional many to many architecture\n",
    "\n",
    "이 아키텍처에서 우리는 각 입력 단어를 128 차원 벡터로 embed 할 것이다. 그 결과 출력 벡터의 shape은 (batch_size, 128, 17) 이다.\n",
    "\n",
    "이 버전에서 입력데이터가 17개의 time step을 가지고 있고 출력 데이터 또한 17개의 time step을 가지는 시나리오를 테스트하기 위해서 이 작업을 수행한다.\n",
    "\n",
    "*여기서 17은 전체 문장 중에 최대 길이를 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cQdKVP9Z9IE-"
   },
   "source": [
    "### 1. input과 output 데이터셋 생성\n",
    "\n",
    "'decoder_input_data' 및 'decoder_target_data'가 있다\n",
    "\n",
    "우선, target 문장 단어에 해당하는 단어 ID로 'decoder_input_data'를 생성한다.\n",
    "\n",
    "'decoder_target_data'는 'start' 토큰 이후 모든 워드에 대해 one-hot-encode된 target 데이터 버전이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70CY84YG-rLs"
   },
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros((len(lines.eng), fr_max_length), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(lines.fr), fr_max_length), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(lines.fr), fr_max_length, num_decoder_tokens+1), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 17)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 17)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 17, 359)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PNeNkX0e_UnJ"
   },
   "source": [
    "> 이전 섹션의 7b 단계에서 만든 사전에는 index 0에 해당하는 단어가 없으므로 'num_decodder_tokens'에 +1을 추가하고 있다는 점에 유의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_6YTFU8_f9T"
   },
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(lines.eng, lines.fr)):\n",
    "    \n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = input_token_index[word]\n",
    "        \n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = target_token_index[word]\n",
    "        if t > 0:          \n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            if t == len(target_text.split()) - 1:\n",
    "                decoder_target_data[i, t:, 89] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index['end']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgviNRDc_p8i"
   },
   "source": [
    "영어 또는 프랑스어로 된 문장을 영어와 프랑스어로 대응하는 단어 ID로 대체하기 위해 입력 텍스트와 대상 텍스트를 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8piTNVn-ACOT"
   },
   "source": [
    "이 결과를 모델에 전달할 수 있도록 디코더에 있는 target 데이터를 한 번에 인코딩하고 있다. \n",
    "\n",
    "또한, 모든 문장의 길이가 현재 동일하다는 점을 감안하여, 다음과 같은 루프에 문장 길이가 초과 된 후 89번째 색인(89는 최종 색인에 속함)에서 대상 데이터의 값을 1로 대체하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 17) (50000, 17) (50000, 17, 359)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input_data.shape,encoder_input_data.shape,decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[120.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [264.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [264.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [336.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [336.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[284., 321.,  89.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [284., 320.,  89.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [284., 320.,  89.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [284., 352., 320.,  89.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [284.,  18., 320.,  89.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtkxNFwyAU-i"
   },
   "outputs": [],
   "source": [
    "for i in range(decoder_input_data.shape[0]):\n",
    "    for j in range(decoder_input_data.shape[1]):\n",
    "        if(decoder_input_data[i][j]==0):\n",
    "            decoder_input_data[i][j] = 89"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBfAqzBUAfJI"
   },
   "source": [
    "앞의 코드에서는 디코더 입력 데이터의 0 값을 89로 대체하고 있다(89는 엔딩 토큰이고 0은 우리가 만든 단어 인덱스와 관련된 단어가 없기 때문에)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index['end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 577,
     "status": "ok",
     "timestamp": 1563435291274,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "QUCpSuYDAUgT",
    "outputId": "bb1f7a6f-f7a5-42a5-aead-2197047f292d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 17) (50000, 17) (50000, 17, 359)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input_data.shape, encoder_input_data.shape, decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1563435424501,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "AXnwZNNtA1FS",
    "outputId": "c0673328-2cb4-4dd5-8528-b12b96b5fe90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([284., 321.,  89.,  89.,  89.,  89.,  89.,  89.,  89.,  89.,  89.,\n",
       "        89.,  89.,  89.,  89.,  89.,  89.], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 661,
     "status": "ok",
     "timestamp": 1563435429346,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "_liBucAdA1ez",
    "outputId": "12d91654-2743-4042-a7dd-90d44f63e57c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([321,  89,  89,  89,  89,  89,  89,  89,  89,  89,  89,  89,  89,\n",
       "        89,  89,  89,  89])"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(decoder_target_data[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1563435443060,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "JHVRlYodA476",
    "outputId": "a4e835bf-8349-42dd-b00f-31b8976184a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([284., 320.,  89.,  89.,  89.,  89.,  89.,  89.,  89.,  89.,  89.,\n",
       "        89.,  89.,  89.,  89.,  89.,  89.], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 674,
     "status": "ok",
     "timestamp": 1563435450502,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "Wtpj1aB4A6is",
    "outputId": "6fdf2996-90e3-4ff4-b7a3-fcdb4c03ed32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([320,  89,  89,  89,  89,  89,  89,  89,  89,  89,  89,  89,  89,\n",
       "        89,  89,  89,  89])"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(decoder_target_data[1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sqx52s5UAyIs"
   },
   "source": [
    "### 2. Build and fit the model, as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAy3TvmzA-4l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, ZeroPadding2D, BatchNormalization, Input, Dropout\n",
    "from keras.layers import Conv2DTranspose, Reshape, Activation, Cropping2D, Flatten\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, RepeatVector, TimeDistributed, Dense, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2077,
     "status": "ok",
     "timestamp": 1563435475722,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "2jzngOCLBBGT",
    "outputId": "4723c947-82f2-41d1-8f15-173e0127eb89"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0718 07:37:53.316706 139670635399040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0718 07:37:53.338088 139670635399040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0718 07:37:53.342557 139670635399040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0718 07:37:53.894759 139670635399040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 17, 128)           49408     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 17, 512)           788480    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 17, 256)           787456    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 17, 359)           92263     \n",
      "=================================================================\n",
      "Total params: 1,717,607\n",
      "Trainable params: 1,717,607\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define NMT model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(input_words)+1, 128, input_length=fr_max_length, mask_zero=True))\n",
    "model.add((Bidirectional(LSTM(256, return_sequences = True))))\n",
    "#model.add(RepeatVector(fr_max_length))\n",
    "model.add((LSTM(256, return_sequences=True)))\n",
    "model.add((Dense(len(target_token_index)+1, activation='softmax')))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 749,
     "status": "ok",
     "timestamp": 1563435486073,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "6QTkIDOpA_Nj",
    "outputId": "ddd22fcc-4202-4c26-9fd9-9ffd13b60124"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0718 07:38:05.047342 139670635399040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0718 07:38:05.083129 139670635399040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1550979,
     "status": "ok",
     "timestamp": 1563437042072,
     "user": {
      "displayName": "K J",
      "photoUrl": "https://lh4.googleusercontent.com/-B2txY1D14ok/AAAAAAAAAAI/AAAAAAAAAEw/4KV8cEiIlaU/s64/photo.jpg",
      "userId": "03106099213865703675"
     },
     "user_tz": -540
    },
    "id": "fm99LuQSBEf6",
    "outputId": "97d5f2bc-501a-4f4b-de0e-68311c12347c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0718 07:38:13.234614 139670635399040 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "47500/47500 [==============================] - 312s 7ms/step - loss: 3.5412 - acc: 0.3202 - val_loss: 3.1503 - val_acc: 0.3796\n",
      "Epoch 2/5\n",
      "47500/47500 [==============================] - 310s 7ms/step - loss: 2.5286 - acc: 0.4577 - val_loss: 2.6579 - val_acc: 0.4320\n",
      "Epoch 3/5\n",
      "47500/47500 [==============================] - 311s 7ms/step - loss: 2.1258 - acc: 0.5048 - val_loss: 2.3858 - val_acc: 0.4555\n",
      "Epoch 4/5\n",
      "47500/47500 [==============================] - 309s 6ms/step - loss: 1.8828 - acc: 0.5395 - val_loss: 2.2154 - val_acc: 0.4800\n",
      "Epoch 5/5\n",
      "47500/47500 [==============================] - 306s 6ms/step - loss: 1.7114 - acc: 0.5647 - val_loss: 2.1236 - val_acc: 0.4874\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(encoder_input_data, decoder_target_data,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yP7k_p0PBLJy"
   },
   "outputs": [],
   "source": [
    "history_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_I87MLWCifw"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "epochs = range(1, len(val_loss_values) + 1)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(epochs, history.history['loss'], 'ro', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Test loss')\n",
    "plt.title('Training and test loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(epochs, history.history['acc'], 'ro', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Test accuracy')\n",
    "plt.title('Training and test accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()]) \n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vyaFBurACpgW"
   },
   "outputs": [],
   "source": [
    "encoder_input_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gqz-w2XiBLgH"
   },
   "source": [
    "모델의 정확도 숫자는 정확도 측정에서도 'end' 토큰을 계산하기 때문에 오해의 소지가 있다는 점에 유의하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hvJOLHfbBX4z"
   },
   "source": [
    "3. Calculate the number of words that were correctly translated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ql0bsxCoBZY7"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "correct_count = 0\n",
    "pred = model2.predict(encoder_input_data[47500:])\n",
    "for i in range(2500):\n",
    "    t = np.argmax(pred[i], axis=1)\n",
    "    act = np.argmax(decoder_target_data[47500], axis=1)\n",
    "    correct_count += np.sum((act == t) & (act != 89))\n",
    "    count += np.sum(act != 89)\n",
    "correct_count/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "afdWKPABCtqd"
   },
   "outputs": [],
   "source": [
    "decoder_input_data[47500+i] != 89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2F_qGCyhCt7I"
   },
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wEaXCjQyCxRD"
   },
   "outputs": [],
   "source": [
    "decoder_input_data[47500+i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SvF_45zTBbiA"
   },
   "source": [
    "테스트 데이터(유효성 분할이 5%이므로 총 데이터 집합의 마지막 5%이다)에 대해 예측하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SznPWvESBbXT"
   },
   "source": [
    " 전체 단어의 ~19%가 정확하게 번역되었음을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgFbcegoBpHk"
   },
   "source": [
    "## Many to hidden to many architecture\n",
    "\n",
    "이전 아키텍처의 단점 중 하나는 입력에 어떤 입력이 있는 곳에서 최대 8개의 time step가 있다는 것을 알면서도 입력의 time step 수를 인위적으로 17개로 늘려야 한다는 것이었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iABUmqIUCGRU"
   },
   "source": [
    "이 아키텍처에서는 입력의 마지막 단계에서 숨겨진 상태 값을 추출하는 모델을 구축해 봅시다. \n",
    "\n",
    "또한 은닉 상태 값을 17회 복제한다(출력에는 17개의 시간 단계가 있으므로). \n",
    "\n",
    "복제된 숨겨진 시간 단계를 Dense 계층을 통과하여 출력에 있을 수 있는 클래스를 최종적으로 추출한다. 논리를 다음과 같이 코드화하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNibq4JYCNA_"
   },
   "source": [
    "### 1. 입력에 8 time step, 출력에 17 time step이 되도록 입력 및 출력 데이터셋 생성\n",
    "\n",
    "이는 입력에 17개의 time step이 있었고 현재 버전에서 8개의 time step이 있었기 때문에 이전 반복과는 다르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ANlSHRFCfpl"
   },
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros((len(lines.eng), eng_max_length), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(lines.fr), fr_max_length), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(lines.fr), fr_max_length, num_decoder_tokens+1), dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(lines.eng, lines.fr)):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = input_token_index[word]\n",
    "        \n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = target_token_index[word]\n",
    "        if t>0:          \n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[word]] = 1\n",
    "            if t == len(target_text.split()) - 1:\n",
    "                decoder_target_data[i, t:, 89] = 1\n",
    "\n",
    "for i in range(decoder_input_data.shape[0]):\n",
    "    for j in range(decoder_input_data.shape[1]):\n",
    "        if(decoder_input_data[i][j]==0):\n",
    "            decoder_input_data[i][j] = 89"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TfVj-UZ5DDqF"
   },
   "source": [
    "### 2. Build the model. \n",
    "\n",
    "RepeatVector 계층은 양방향 계층의 출력을 17회 복제한다는 점에 유의하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SeGAO-L-DcX2"
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(len(input_words)+1, 128, input_length=eng_max_length, mask_zero=True))\n",
    "model2.add((Bidirectional(LSTM(256))))\n",
    "model2.add(RepeatVector(fr_max_length))\n",
    "model2.add((LSTM(256, return_sequences=True)))\n",
    "model2.add((Dense(len(target_token_index)+1, activation='softmax')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bFczen__DjKJ"
   },
   "source": [
    "### 3. Compile and fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6RTuheTDlVF"
   },
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
    "history1 = model2.fit(encoder_input_data, decoder_target_data,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hebUXrxeDvwv"
   },
   "outputs": [],
   "source": [
    "history_dict = history1.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "epochs = range(1, len(val_loss_values) + 1)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "plt.subplot(211)\n",
    "plt.plot(epochs, history1.history['loss'], 'ro', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Test loss')\n",
    "plt.title('Training and test loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(epochs, history1.history['acc'], 'ro', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Test accuracy')\n",
    "plt.title('Training and test accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()]) \n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fe2k2B8zDxuL"
   },
   "outputs": [],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VBiwt-V-DqKc"
   },
   "source": [
    "### 4. Calculate the % of total words that are correctly translated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o45Cl8liDsKU"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "correct_count = 0\n",
    "pred = model2.predict(encoder_input_data[47500:])\n",
    "for i in range(2500):\n",
    "  t = np.argmax(pred[i], axis=1)\n",
    "  act = np.argmax(decoder_target_data[47500],axis=1)\n",
    "  correct_count += np.sum((act==t) & (act!=89))\n",
    "  count += np.sum(act!=89)\n",
    "correct_count/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TGElDIP7D5P6"
   },
   "source": [
    "앞의 결과는 정확도가 19%로 이전의 반복과 거의 대등하다.\n",
    "\n",
    "모든 입력 시간 단계의 정보가 마지막 숨겨진 계층 값에만 저장되었을 때 상당한 양의 정보가 손실되는 경향이 있기 때문에, 선행은 기대된다.\n",
    "\n",
    "또한, 우리는 어느 시간 단계에서 잊혀져야 할 것에 대한 상당한 양의 정보를 포함하는 세포 상태를 사용하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d049uuZdD-8Y"
   },
   "source": [
    "## Encoder decoder architecture for machine translation\n",
    "\n",
    "이전 섹션에서 정의한 아키텍처에는 두 가지 잠재적인 논리적 향상 기능이 있다.\n",
    "\n",
    "1.번역을 생성하는 동안 셀 상태에 존재하는 정보를 활용한다.\n",
    "\n",
    "2. 앞서 번역한 단어를 다음 단어의 예측에 입력으로 활용\n",
    "\n",
    "두 번째 기법은 **Teacher Forcing**(교사 위조)라고 불린다. \n",
    "\n",
    "본질적으로 이전 시간 단계의 실제 값을 입력으로 주면서 현재 시간 단계를 생성함으로써 네트워크를 보다 빠르고, 실질적으로 더 정확하게 튜닝하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JaxsjvBVEa5u"
   },
   "source": [
    "### Getting ready\n",
    "\n",
    "기계 번역 시스템을 구축하기 위해 채택할 인코더 디코더 아키텍처는 다음과 같다.\n",
    "\n",
    "- 두 개의 decoder 데이터셋\n",
    "    - `encoder_input_data`와 결합된 **decoder_input_data**가 입력이고 **decoder_target_data**가 출력이다.\n",
    "\n",
    "  - **decoder_input_data**는 `start` 단어로 시작된다.\n",
    "\n",
    "- 우리가 decoder에서 첫번째 단어를 예측할 때, 우리는 단어들의 input set을 벡터로 변환하고, 그 다음 input으로서 `start`를 decoder 모델에 통과시킨다. 예상되는 output은 output에서 `start`뒤의 첫 번째 단어이다.\n",
    "\n",
    "- 비슷한 방식으로 output의 실제 첫 번째 단어가 input으로 두 번째 단어를 예측한다.\n",
    "\n",
    "- 두 번째 단어를 예측하면서 출력의 실제 첫 단어가 입력인 유사한 방식으로 진행한다.\n",
    "\n",
    "- 이 전략을 바탕으로 모델의 정확성을 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3cM54VxE-NX"
   },
   "source": [
    "### How to do it...\n",
    "\n",
    "이를 통해, 이전 섹션에서 준비한 입력 및 출력 데이터셋에 대한 모델을 구축하자(이전 섹션의 아키텍처와 1단계는 동일)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0EOtBZfWFGFg"
   },
   "source": [
    "#### 1. Build the model, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eqpFHEdSFKNJ"
   },
   "outputs": [],
   "source": [
    "# We shall convert each word into a 128 sized vector\n",
    "embedding_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BcgzXO8WFLI0"
   },
   "source": [
    "##### 1) Prepare the encoder model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ngwtRqE4FLSF"
   },
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(eng_max_length,))\n",
    "en_x=  Embedding(num_encoder_tokens+1, embedding_size)(encoder_inputs)\n",
    "en_x = Dropout(0.1)(en_x)\n",
    "encoder = LSTM(256, return_sequences=True, unroll=True)(en_x)\n",
    "encoder_last = encoder[:,-1,:]\n",
    "\n",
    "print('encoder', encoder)\n",
    "print('encoder_last', encoder_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ivl-J_0ad7fr"
   },
   "source": [
    "인코더 네트워크의 중간 계층을 추출하고 있으며 여러 데이터셋을 입력(인코더 입력 데이터 및 디코더 입력 데이터)으로 전달하기 때문에 기능 API를 사용하고 있다는 점에 유의하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQ7muAXoeI_y"
   },
   "source": [
    "##### 2. Prepare the decoder model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIc0i7t1eMPS"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(fr_max_length,))\n",
    "\n",
    "dex=  Embedding(num_decoder_tokens+1, embedding_size)\n",
    "#dex = Dropout(0.1)(dex)\n",
    "\n",
    "decoder= dex(decoder_inputs)\n",
    "decoder = Dropout(0.1)(decoder)\n",
    "\n",
    "decoder = LSTM(256, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n",
    "print('decoder', decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y2TEmRYgeQg5"
   },
   "source": [
    "#### 2. Build the model, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hLdH3llFeRPN"
   },
   "outputs": [],
   "source": [
    "model3 = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etXefya4ea-8"
   },
   "source": [
    "#### 3. Fit the model, as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kq7dH2ekegOj"
   },
   "outputs": [],
   "source": [
    "history3 = model3.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=32,epochs=5,validation_split=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Yzcbn-5ejmU"
   },
   "source": [
    "#### 4. Calculate the % of words that are accurately transcribed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NqHGnWnkelAN"
   },
   "outputs": [],
   "source": [
    "act = np.argmax(decoder_target_data, axis=2)\n",
    "\n",
    "count = 0\n",
    "correct_count = 0\n",
    "pred = model3.predict([encoder_input_data[47500:],decoder_input_data[47500:]])\n",
    "\n",
    "for i in range(2500):\n",
    "    t = np.argmax(pred[i], axis=1)\n",
    "    correct_count += np.sum((act[47500+i] == t) & (act[47500+i] != 0))\n",
    "    count += np.sum(decoder_input_data[47500+i] != 0)\n",
    "correct_count/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ert8gc1flxA"
   },
   "source": [
    "이 시나리오에서 전체 단어의 44%를 정확하게 번역했다는 점에 유의하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Nh0jZ4vfvL3"
   },
   "source": [
    "그러나 실시간 시나리오에서는 이 기능에 액세스할 수 없으므로 테스트 데이터 세트의 정확도를 계산할 때 'decoder_input_data'를 사용해서는 안 된다는 점에 유의하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oZb8QPSf4bN"
   },
   "source": [
    "이것은 우리가 이전 시간 단계에서 예측된 단어를 현재 시간 단계의 디코더 입력 단어로 사용할 것을 요구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgeoxYNTf9yT"
   },
   "source": [
    "'decoder_input_data'를 'decoder_input_data_preed'로 다시 초기화하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Eqiz9SxgD7g"
   },
   "outputs": [],
   "source": [
    "decoder_input_data_pred = np.zeros((len(lines.fr),fr_max_length),dtype='float32')\n",
    "final_pred = []\n",
    "for i in range(2500):\n",
    "    word = 284\n",
    "    for j in range(17):\n",
    "        decoder_input_data_pred[(47500+i), j] = word\n",
    "        pred = model3.predict([encoder_input_data[(47500+i)].reshape(1,8), decoder_input_data_pred[47500+i].reshape(1,17)])\n",
    "        t = np.argmax(pred[0][j])\n",
    "        word = t\n",
    "        if word == 89:\n",
    "            break\n",
    "    final_pred.append(list(decoder_input_data_pred[47500+i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFz33XARjTww"
   },
   "source": [
    "앞의 코드에서 index 284는 `start`단어에 해당한다.\n",
    "\n",
    "우리는 decoder input의 첫 번째 단어로 `start` 단어를 전달하고 다음 time step에 가장 높은 확률를 가진 단어를 예측한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAoq8IXAju78"
   },
   "source": [
    "두 번째 단어를 예측하면, `decoder_input_word_preed`를 업데이트하고, 세 번째 단어를 예측하고, ... `stop` 단어가 등장할 때 까지 계속한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hym8UTPrj_FN"
   },
   "source": [
    "이제 예상 번역된 단어를 수정했으므로, 번역의 정확성을 계산해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AIuR1zPFkDw8"
   },
   "outputs": [],
   "source": [
    "final_pred2 = np.array(final_pred)\n",
    "count = 0\n",
    "correct_count = 0\n",
    "for i in range(2500):\n",
    "    correct_count += np.sum((decoder_input_data[47500+i]==final_pred2[i]) & (decoder_input_data[47500+i]!=89))\n",
    "    count += np.sum(decoder_input_data[47500+i]!=89)\n",
    "correct_count/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFk-1qNGkNDG"
   },
   "source": [
    "앞의 결과는 이 방법을 통해 정확하게 번역되는 모든 단어의 46%를 가져온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pw1WlwaZkNuI"
   },
   "source": [
    "이전의 방법에서 번역의 정확성이 상당히 향상되어 있는 반면, 우리는 여전히 소스 언어에서 출발하는 단어들은 처음부터, 즉 대상 언어에서도, 즉 정렬이라는 단어를 고려하지 않을 수 있다는 직관을 취하고 있지 않다. \n",
    "\n",
    "다음 절에서는 단어 정렬의 문제를 해결하는 것을 검토할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F2FRggcykWyp"
   },
   "source": [
    "## Encoder decoder architecture with attention for machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aDciEaD1kaCp"
   },
   "source": [
    "앞의 절에서는, 목표의 이전 시간 단계의 실제 단어가 모델에 대한 입력으로 사용되는 교사 강제 기법을 가능하게 함으로써 번역의 정확성을 높일 수 있다는 것을 배웠다.\n",
    "\n",
    "이 절에서는 이 아이디어를 더 확장하고 각 단계에서 인코더와 디코더 벡터가 얼마나 유사한지에 기초하여 입력 인코더에 가중치를 할당한다. \n",
    "\n",
    "이런 식으로, 우리는 특정 단어가 암호기의 시간 단계에 따라 암호기의 숨겨진 벡터에 더 높은 가중치를 가질 수 있도록 하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0hL3iFpOknY0"
   },
   "source": [
    "### How to do it...\n",
    "\n",
    "이것으로, 주의 메커니즘과 함께 인코더 디코더 아키텍처를 구축하는 방법을 살펴보자.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qCS2mJCQkwhW"
   },
   "source": [
    "#### 1. Build the encoder, as shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1OOt-wsbk0xp"
   },
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(eng_max_length,))\n",
    "en_x= Embedding(num_encoder_tokens+1, embedding_size)(encoder_inputs)\n",
    "en_x = Dropout(0.1)(en_x)\n",
    "encoder = LSTM(256, return_sequences=True, unroll=True)(en_x)\n",
    "encoder_last = encoder[:,-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qEgVc6Krk5bd"
   },
   "source": [
    "#### 2. Build the decoder, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsbHb0AEk8iE"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(fr_max_length,))\n",
    "dex= Embedding(num_decoder_tokens+1, embedding_size)\n",
    "decoder= dex(decoder_inputs)\n",
    "decoder = Dropout(0.1)(decoder)\n",
    "decoder = LSTM(256, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hjzhWtxXlCyW"
   },
   "source": [
    "앞의 코드에서, 우리는 디코더 아키텍처를 완성하지 못했다.\n",
    "우리는 디코더에서 숨겨진 레이어 값을 추출했을 뿐이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XA1TUNU9lE9a"
   },
   "source": [
    "#### 3. Build the attention mechanism. \n",
    "\n",
    "주의 메커니즘은 각 단계에서 숨겨진 벡터와 디코더 숨겨진 벡터가 얼마나 유사한지에 기초한다. \n",
    "\n",
    "이 유사성(가능한 모든 입력 시간 단계에서 최대 1개까지 합한 가중치를 제공하기 위해 수행되는 소프트맥스)에 기초하여, 다음과 같이 인코더 벡터에 가중치를 할당한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9IgCYxgXlR3_"
   },
   "source": [
    "활성화 및 밀도 레이어를 통해 인코더 디코더 벡터를 전달하여 벡터 간에 도트 제품(비유사한 점—비유사한 점의 척도)을 취하기 전에 추가적인 비선형성을 달성하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6gD6nGAalTyq"
   },
   "outputs": [],
   "source": [
    "t = Dense(5000, activation='tanh')(decoder)\n",
    "t2 = Dense(5000, activation='tanh')(encoder)\n",
    "attention = dot([t, t2], axes=[2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zh51C_f7lWQq"
   },
   "source": [
    "입력 시간 단계에 부여할 가중치 식별:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jBZ3n7vllWrd"
   },
   "outputs": [],
   "source": [
    "attention = Dense(eng_max_length, activation='tanh')(attention)\n",
    "attention = Activation('softmax')(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAeqCJ0alYcC"
   },
   "source": [
    "가중 인코더 벡터를 다음과 같이 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61zVuJPrlauP"
   },
   "outputs": [],
   "source": [
    "context = dot([attention, encoder], axes = [2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bqhSCJ4tlb47"
   },
   "source": [
    "#### 4. Combine the decoder and weighted encoder vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Gluy4sWleaJ"
   },
   "outputs": [],
   "source": [
    "decoder_combined_context = concatenate([context, decoder])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jBI4x1Q-ldUx"
   },
   "source": [
    "#### 5. Connect the combination of decoder and weighted encoded vector to output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jCZ_mX7Llh42"
   },
   "outputs": [],
   "source": [
    "output_dict_size = num_decoder_tokens + 1\n",
    "decoder_combined_context = Dense(2000, activation='tanh')(decoder_combined_context)\n",
    "output=(Dense(output_dict_size, activation=\"softmax\"))(decoder_combined_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9xiMDuMlgpG"
   },
   "source": [
    "#### 6. Compile and fit the model, shown in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "26ah1RJSloV_"
   },
   "outputs": [],
   "source": [
    "model4 = Model(inputs=[encoder_inputs, decoder_inputs], outputs=[output])\n",
    "model4.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3-p1mWE1lrvp"
   },
   "outputs": [],
   "source": [
    "model4.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=32, epochs=5, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVCSRhu3lny5"
   },
   "source": [
    "모형을 적합시키면 이 모델의 유효성 검사 손실이 이전 반복보다 약간 낫다는 것을 알게 될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bFVSKxlplx9r"
   },
   "source": [
    "#### 7. Calculate the accuracy of translation in a similar way to what we did in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAtURYbUl1hg"
   },
   "outputs": [],
   "source": [
    "decoder_input_data_pred=np.zeros((len(lines.fr), fr_max_length), dtype='float32')\n",
    "\n",
    "final_pred_att = []\n",
    "for i in range(2500):\n",
    "  word = 284\n",
    "  for j in range(17):\n",
    "    decoder_input_data_pred[(47500+i), j] = word\n",
    "    pred = model4.predict([encoder_input_data[(47500+i)].reshape(1,8),decoder_input_data_pred[47500+i].reshape(1,17)])\n",
    "    t = np.argmax(pred[0][j])\n",
    "    word = t\n",
    "    if word==89:\n",
    "      break\n",
    "  final_pred_att.append(list(decoder_input_data_pred[47500+i]))      \n",
    "  \n",
    "final_pred2_att = np.array(final_pred_att)\n",
    "count = 0\n",
    "correct_count = 0\n",
    "for i in range(2500):\n",
    "  correct_count += np.sum((decoder_input_data[47500+i]==final_pred2_att[i]) & (decoder_input_data[47500+i]!=89))\n",
    "  count += np.sum(decoder_input_data[47500+i]!=89)\n",
    "correct_count/count  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uxe3hzKXl0H_"
   },
   "source": [
    "앞의 코드는 정확히 번역된 전체 단어의 52%를 낳는데, 이는 이전의 반복보다 개선된 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1SVFn9imUnj"
   },
   "source": [
    "#### 8. 합리적인 정확도를 가진 변환 시스템을 구축했으므로, 다음과 같이 테스트 데이터 세트의 몇 가지 변환(테스트 데이터 세트는 검증_분할을 5%로 지정한 총 데이터 세트의 마지막 5%임)을 검사해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "17GLNSWXmgaH"
   },
   "outputs": [],
   "source": [
    "k = -1500\n",
    "t = model4.predict([encoder_input_data[k].reshape(1,encoder_input_data. shape[1]),decoder_input_data[k].reshape(1,decoder_input_data.shape[1])]).reshape(decoder_input_data.shape[1], num_decoder_tokens+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PzDcxIXgmdu8"
   },
   "source": [
    "Extract the predicted translations in terms of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jM8jyQdYmlK0"
   },
   "outputs": [],
   "source": [
    "t2 = np.argmax(t,axis=1)\n",
    "for i in range(len(t2)):\n",
    "  if int(t2[i])!=0:\n",
    "    print(list(target_token_index.keys())[int(t2[i]-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ByXY1dO_mnpA"
   },
   "source": [
    "> The output of the preceding code after converting English sentence to French is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFKWs2PDmtlv"
   },
   "source": [
    "Extract the actual translations in terms of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qsjlb5uYmuNV"
   },
   "outputs": [],
   "source": [
    "t2 = decoder_input_data[k]\n",
    "for i in range(len(t2)):\n",
    "  if int(t2[i])!=89:\n",
    "    print(list(target_token_index.keys())[int(t2[i]-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yKnDV1Emm0XN"
   },
   "source": [
    "우리는 예측된 번역이 원래의 번역에 상당히 가깝다는 것을 안다. \n",
    "\n",
    "이와 유사한 방식으로 검증 데이터 세트에 대해 몇 가지 더 자세히 알아보도록 합시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wLvaQEbm6b8"
   },
   "source": [
    "앞에서 우리는 괜찮은 번역이 있다는 것을 알 수 있지만, 몇 가지 잠재적 개선 영역이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2vcgch4Lm8fB"
   },
   "source": [
    "- 단어 유사성에 대한 설명:\n",
    "  - 제나 제이와 같은 단어들은 상당히 비슷하기 때문에 정확도를 떨어뜨리고 있음에도 불구하고 심하게 처벌되어서는 안 된다\n",
    "  \n",
    "- unk 수 감소:\n",
    "  - 데이터 세트의 차원을 줄이기 위해 Unk 단어 수를 줄임\n",
    "  - 더 큰 코퍼스를 수집하고 산업 규모 구성을 갖춘 기계에서 작업할 때, 우리는 잠재적으로 높은 치수 데이터를 작업할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhl3NQqGnSKs"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Machine translation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
